# Stable Diffusion - Image to Prompts

![SD](https://github.com/user-attachments/assets/05b5635c-45fd-43a9-b391-e704590e9e3a)

## Background
The aim is to reverse the typical direction of a generative text-to-image model: instead of generating an image from a text prompt, can you create a model which can predict the text prompt given a generated image? You will make predictions on a dataset containing a wide variety of (prompt, image) pairs generated by Stable Diffusion 2.0, in order to understand how reversible the latent relationship is.
﻿
Your task for this challenge is to predict the prompts that were used to generate target images. Prompts for this challenge were generated using a variety of (non disclosed) methods, and range from fairly simple to fairly complex with multiple objects and modifiers. Images were generated from the prompts using Stable Diffusion 2.0 (768-v-ema.ckpt) and were generated with 50 steps at 768x768 px and then downsized to 512x512 for the competition dataset. (This script was used, with the majority of default parameters unchanged.)

比赛目标
这场比赛的目标是扭转生成性文本到图像模型的典型方向：你能创建一个模型来预测给定生成图像的文本提示，而不是从文本提示生成图像吗？您将对包含Stable Diffusion 2.0生成的各种（提示、图像）对的数据集进行预测，以了解潜在关系的可逆性。

您的任务是预测用于生成目标图像的提示。该挑战的提示是使用各种（未公开）方法生成的，从相当简单到相当复杂，有多个对象和修改器。图像是使用Stable Diffusion 2.0（768-v-ema.ckpt）从提示中生成的，在768x768 px下以50步生成，然后缩小到512x512，用于比赛数据集。（使用此脚本时，大多数默认参数保持不变。）
