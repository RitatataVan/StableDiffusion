{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install /kaggle/input/stable-diffusion-timm/safetensors-0.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install /kaggle/input/stable-diffusion-timm/timm-0.8.17.dev0-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install /kaggle/input/stable-diffusion-timm/ftfy-6.1.1-py3-none-any.whl\n#!pip install /kaggle/input/stable-diffusion-timm/open_clip_torch-2.16.0-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom pathlib import Path\nimport open_clip\nfrom transformers import AutoModel, AutoProcessor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/images/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png')) \nfor path in images:\n    Image.open(path).save(f'/kaggle/images/{path.stem}.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\ndef _convert_to_rgb(image):\n    return image.convert('RGB')\n#clip_processor = AutoProcessor.from_pretrained(MODEL)\nclip_model, _, _ = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained=None)#\"layers\": 32\nclip_processor = transforms.Compose([\n    transforms.Resize(512, interpolation=transforms.InterpolationMode.BICUBIC),\n    transforms.CenterCrop(512),\n    #transforms.RandomHorizontalFlip(p=1.0),\n    _convert_to_rgb, \n    transforms.ToTensor(),\n    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711]),])","metadata":{"execution":{"iopub.status.busy":"2023-05-08T15:36:09.361637Z","iopub.execute_input":"2023-05-08T15:36:09.362576Z","iopub.status.idle":"2023-05-08T15:36:11.799469Z","shell.execute_reply.started":"2023-05-08T15:36:09.362520Z","shell.execute_reply":"2023-05-08T15:36:11.798028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = '/kaggle/input/huggingface-vision-models/CLIP-ViT-L-14-laion2B-s32B-b82K/'\n#clip_processor = AutoProcessor.from_pretrained(MODEL)\nBATCHSIZE=16\nSAVE_OPT_CKP = False\nSAVE_MODEL_CKP = True\nUNFREEZE_START = 18 # set it to lower number when significantly more samples are included.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n#clip_model, _, clip_processor = open_clip.create_model_and_transforms('convnext_large_d', pretrained=None)\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        clip = clip_model\n        self.vision = clip.visual\n        self.fc = nn.Linear(1024, 384)\n\n    def forward(self, x):\n        out = self.vision(x)\n        return self.fc(out)\n\n\ndef load_pretrained_model():\n    model = Net()\n    return model.to(device)\n\n\nclass IMGDataset:\n    def __init__(self, image_paths, clip_processor=clip_processor):\n        self.images = image_paths\n        self.input_processor = clip_processor\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, item):\n        image = Image.open(self.images[item])\n        image = self.input_processor(image)\n        return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = list(Path('/kaggle/images').glob('*.jpg')) \nmodel = Net()\nmodel.load_state_dict(torch.load('/kaggle/input/stablediffusion-models/train_convnext_xxlarge_littleboat_on1761k_size512_f0_cv0.6569_ep0_unfree24.pt'))\n\nmodel.to(device)\nmodel.eval()\ntest_dataloader = DataLoader(dataset=IMGDataset(images),\n                             batch_size=BATCHSIZE, shuffle=False, num_workers=2)\npreds = []\nwith torch.no_grad():\n    for batch_images in tqdm(test_dataloader):\n        batch_images = batch_images.to(device)\n\n        pred = model(batch_images)\n        preds.append(pred.cpu().numpy())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimgIds = [i.stem for i in images]\nEMBEDDING_LENGTH = 384\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, EMBEDDING_LENGTH),\n        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n\nprompt_embeddings = np.vstack(preds).flatten()#predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)\nsubmission = pd.DataFrame(\n    index=imgId_eId,\n    data=prompt_embeddings,\n    columns=['val']\n).rename_axis('imgId_eId')\nsubmission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}